Our TextID Model identified the winner in each category (words, word lengths, sentence lengths, stems, and punctuation), and chose the better matched model by identifying the one with the more wins from each category. We tested our model in two different ways. The first compared large texts - the Hunger Games Book and a Motivational Book against Saya's Writ01 Essay. We found that it was more similar to the Motivational Book. The second test compared Laney and Rohan's Writ01 Essays against both of their reflective essays. Our model correctly identified each reflective essay to their original author! We were very impressed with the accuracy of the model. In fact, all categories supported the same conclusion that Laney's writing style was the same in her main and reflective essays. This shows the sensitivity of the stylometrics: while all of us followed the same writing principles in terms of concision, we still preserved our individuality. However, it is important to note the model was more accurate while comparing texts on the same subject. Both authorship and subject matter affect the accuracy. In the future, we hope to apply this to longer text files and compare authors in non-English languages.